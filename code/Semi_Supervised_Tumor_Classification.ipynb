{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKBUveNkVK1i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Activation, Conv2DTranspose, Conv2D, MaxPooling2D, Input, UpSampling2D, Dense, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.image as img\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from imblearn.metrics import geometric_mean_score,specificity_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, precision_recall_curve\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_yes ='../input/brain-mri-images-for-brain-tumor-detection/yes/'\n",
        "train_no ='../input/brain-mri-images-for-brain-tumor-detection/no/'\n",
        "#train_pred ='../input/brain-tumor-detection/pred/'"
      ],
      "metadata": {
        "id": "KXxg5T8VonBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total Tumor images:', len(os.listdir(train_yes)))\n",
        "print('Total Non-Tumor images:', len(os.listdir(train_no)))"
      ],
      "metadata": {
        "id": "mWg6ZzrpooAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-Tumor: 0 Tumor: 1"
      ],
      "metadata": {
        "id": "-MAr9bwhoyYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_yes = np.empty(155); y_train_yes.fill(1)\n",
        "y_train_no = np.empty(98); y_train_no.fill(0)"
      ],
      "metadata": {
        "id": "JaJRuhLBoqXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yes_train = []\n",
        "for filename in os.listdir(train_yes):\n",
        "    img = image.load_img(train_yes + filename, target_size=(128, 128),color_mode='grayscale')\n",
        "    yes_train.append(image.img_to_array(img))\n",
        "yes_train = np.array(yes_train)\n",
        "\n",
        "no_train = []\n",
        "for filename in os.listdir(train_no):\n",
        "    img = image.load_img(train_no + filename, target_size=(128, 128),color_mode='grayscale')\n",
        "    no_train.append(image.img_to_array(img))\n",
        "no_train = np.array(no_train)"
      ],
      "metadata": {
        "id": "xRgAOOQuouJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.concatenate((yes_train, no_train),axis=0)"
      ],
      "metadata": {
        "id": "OYHmZGnJouFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_data(X, n=5, title=\"\"):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(2,n,i+1)\n",
        "        plt.imshow(image.array_to_img(X[i]),cmap='gray')\n",
        "    #plt.suptitle(title, fontsize = 20)\n",
        "\n",
        "show_data(X_train)"
      ],
      "metadata": {
        "id": "xx30XoPMouDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.concatenate((y_train_yes, y_train_no),axis=0)"
      ],
      "metadata": {
        "id": "oBfp6MagouAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "zCFUncI5ot9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding the image.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "EBgkWqiqot6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape[1:]"
      ],
      "metadata": {
        "id": "ef6fCzYaot37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 432\n",
        "\n",
        "input_layer = Input(shape=(128, 128, 1), name=\"INPUT\")\n",
        "x = Conv2D(12, 3, padding=\"same\", activation='relu')(input_layer) #activation=\"relu\"\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "#x = LeakyReLU()(x)\n",
        "x = Conv2D(6, 3, padding=\"same\", activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "#x = LeakyReLU()(x)\n",
        "'''x = Conv2D(3, 3, padding=\"same\")(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = LeakyReLU()(x)'''\n",
        "x = layers.Flatten()(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(input_layer, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "6iBq2Qbvot00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(encoder, show_shapes=True, to_file='encoder_network.png', dpi=600)"
      ],
      "metadata": {
        "id": "CPYGM8dnotx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(6144, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((32, 32, 6))(x)\n",
        "\n",
        "'''x = layers.Conv2DTranspose(3, 3, padding=\"same\",activation=\"relu\")(x) #activation=\"relu\"\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = LeakyReLU()(x)'''\n",
        "x = layers.Conv2DTranspose(6, 3, padding=\"same\",activation=\"relu\")(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "#x = LeakyReLU()(x)\n",
        "x = layers.Conv2DTranspose(12, 3,  padding=\"same\",activation=\"relu\")(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "#x = LeakyReLU()(x)\n",
        "\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "h730yK5Yotuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(decoder, show_shapes=True, to_file='decoder_network.png', dpi=600)"
      ],
      "metadata": {
        "id": "zg2njByJotrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2) #mean_squared_error binary_crossentropy\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "fpvu9u0toteo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "qZFKhjaHpvAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "when n = 64, i.e., n << d (n= latent_dim, d = orginal_dim); the gradients start exploding Hence, 432 latent dimension has been considered."
      ],
      "metadata": {
        "id": "yD4Fkag6pyjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor = 'reconstruction_loss',\n",
        "                            mode = 'min',\n",
        "                            min_delta = 0,\n",
        "                            patience = 6,\n",
        "                            restore_best_weights = True)\n",
        "\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(learning_rate = 5 * 0.0001))\n",
        "vae.fit(X_train, epochs=500, batch_size = 256,\n",
        "                       shuffle = True,\n",
        "                       callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "1vYq17Shpu1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(vae.history.history['reconstruction_loss'])\n",
        "plt.title('reconstruction loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AnN9fwxvpuqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(vae.history.history['kl_loss'])\n",
        "plt.title('KLD loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0AqBhRFNp6su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(vae.history.history['loss'])\n",
        "plt.title('Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyw8vs9Zp6kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,_,z = vae.encoder.predict(X_train)"
      ],
      "metadata": {
        "id": "T-Pugs4pp6bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.shape"
      ],
      "metadata": {
        "id": "4Xmhp4d1p6St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_encoded_data(X, n=5, height=18, width=24, title=\"\"):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(2,n,i+1)\n",
        "        plt.imshow(X[i].reshape((height,width)),cmap='gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "    plt.suptitle(title, fontsize = 20)\n",
        "\n",
        "show_encoded_data(z, height= 18, width= 24) #height= 18, width= 24)"
      ],
      "metadata": {
        "id": "x-7XjKYip6ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(x):\n",
        "    plt.imshow(image.array_to_img(x),cmap='gray')"
      ],
      "metadata": {
        "id": "yX7fs5XcqHjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(img,encoder,decoder):\n",
        "    \"\"\"Draws original, encoded and decoded images\"\"\"\n",
        "    # img[None] will have shape of (1, 48, 48, 3) which is the same as the model input\n",
        "    code = vae.encoder.predict(img[None])[2]\n",
        "    reco = vae.decoder.predict(code)[0]\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"Original\")\n",
        "    show_image(img)\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Code\")\n",
        "    plt.imshow(code.reshape([code.shape[-1]//24,-1]),cmap='gray')\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.title(\"Reconstructed\")\n",
        "    show_image(reco)\n",
        "    plt.show()\n",
        "\n",
        "for i in range(5):\n",
        "    img = X_train[i]\n",
        "    visualize(img,encoder,decoder)"
      ],
      "metadata": {
        "id": "XhV9ubvoqHYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(z).to_csv('Train_Encoded.csv', index = False)"
      ],
      "metadata": {
        "id": "bX6JJys5qRIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df = pd.read_csv(\"Train_Encoded.csv\")\n",
        "y_df = pd.DataFrame(y_train, columns = ['Class'])\n",
        "Y = y_df.Class\n",
        "X_train_df['Class'] = Y.values\n",
        "X_train_df.to_csv('Labeled_Train_Encoded.csv', index = False)"
      ],
      "metadata": {
        "id": "tz2LwujjqRCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = X_train_df.iloc[:, :-1].to_numpy()"
      ],
      "metadata": {
        "id": "dl2Mh2VkqQ6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df"
      ],
      "metadata": {
        "id": "fH287o9VqQwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv('../input/200-epochs-432sized/Labeled_Train_Encoded.csv')\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "CH2t45PS_onH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['Class'] = data_df['Class'].astype('int')"
      ],
      "metadata": {
        "id": "5_CSZOXoA1y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_X_train = data_df.iloc[:, :-1].to_numpy()\n",
        "initial_y_train = data_df.Class"
      ],
      "metadata": {
        "id": "IeLj01wzBmXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx_train, xx_test, yy_train, yy_test = train_test_split(initial_X_train, initial_y_train,\n",
        "                                                    test_size=0.15,shuffle=True, stratify=initial_y_train.ravel())\n",
        "print(f\"X_train Shape: {xx_train.shape}\\nX_test Shape: {xx_test.shape}\\ny_train Shape: {yy_train.shape}\\ny_test Shape:{yy_test.shape}\")"
      ],
      "metadata": {
        "id": "bk-OB0RRBqtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yy_train.isnull().values.any()"
      ],
      "metadata": {
        "id": "RCDaRr2KBqp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training set: 2550 samples Testing set: 450 samples"
      ],
      "metadata": {
        "id": "vmWw_DITBzOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set_df = pd.DataFrame(xx_train)\n",
        "y_df = pd.DataFrame(yy_train, columns = ['Class'])\n",
        "Y = y_df.Class\n",
        "training_set_df['Class'] = Y.values\n",
        "training_set_df"
      ],
      "metadata": {
        "id": "kIFQLm96BqnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of 2550 Training samples: 10% Labeled samples (i.e. 255) and 90% Unlabeled samples (i.e. 2295)"
      ],
      "metadata": {
        "id": "V8b9R1OOB4U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xx_labeled, xx_unlabeled, yy_labeled, yy_unlabeled = train_test_split(xx_train, yy_train,test_size=0.9 ) #,shuffle=True, stratify= yy_train.ravel())\n",
        "print(f\"X_labeled Shape: {xx_labeled.shape}\\nX_unlabeled Shape: {xx_unlabeled.shape}\\ny_labeled Shape: {yy_labeled.shape}\\ny_unlabeled Shape:{xx_unlabeled.shape}\")"
      ],
      "metadata": {
        "id": "4PkZE_17BqkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_df = pd.DataFrame(xx_labeled)\n",
        "y0_df = pd.DataFrame(yy_labeled, columns = ['Class'])\n",
        "Y0 = y0_df.Class\n",
        "labeled_df['Class'] = Y0.values\n",
        "labeled_df.head()"
      ],
      "metadata": {
        "id": "bPZVX0L9Bqd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_df = pd.DataFrame(xx_unlabeled)\n",
        "y1_df = pd.DataFrame(yy_unlabeled, columns = ['Class'])\n",
        "Y1 = y1_df.Class\n",
        "unlabeled_df['Class'] = Y1.values\n",
        "unlabeled_df.head()"
      ],
      "metadata": {
        "id": "4sDaydWYBqRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_unlabeled = unlabeled_df.drop(['Class'], axis=1)"
      ],
      "metadata": {
        "id": "OJBH-cGCCC_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = labeled_df.iloc[:, :-1]\n",
        "y_train = labeled_df.Class"
      ],
      "metadata": {
        "id": "KYC-XDc8CJMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Labeled X\",len(X_train))\n",
        "print(\"Labeled y\",len(y_train))\n",
        "print(\"Unlabeled X\",len(X_unlabeled))"
      ],
      "metadata": {
        "id": "i-d023SZCJDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_shape=(432,), activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the keras model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit the keras model on the dataset\n",
        "model.fit(xx_train, yy_train, epochs=10, batch_size=10)"
      ],
      "metadata": {
        "id": "RnS_-NmoCI5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(xx_train, yy_train)"
      ],
      "metadata": {
        "id": "Aa_W-IDQCXwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "clf = lgb.LGBMClassifier()\n",
        "clf.fit(xx_train, yy_train)"
      ],
      "metadata": {
        "id": "Uy1rME9kCbWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "clf = AdaBoostClassifier(random_state=0)\n",
        "clf.fit(xx_train, yy_train)"
      ],
      "metadata": {
        "id": "FCQuLaOOCdiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(xx_test)"
      ],
      "metadata": {
        "id": "fZljWBRTCf3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict probabilities for test set\n",
        "#yhat_probs = model.predict(xx_test, verbose=0)\n",
        "# predict crisp classes for test set\n",
        "#y_pred = np.argmax(yhat_probs)\n",
        "\n",
        "\n",
        "# predict probabilities for test set\n",
        "yhat_probs = model.predict(xx_test, verbose=0)\n",
        "# predict crisp classes for test set\n",
        "y_pred = (model.predict(xx_test) > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "id": "kdtdrt4PCk0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(yy_test, y_pred)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(yy_test, y_pred)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(yy_test, y_pred)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(yy_test, y_pred)\n",
        "print('F1 score: %f' % f1)"
      ],
      "metadata": {
        "id": "QyKi9R0gCn1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Baseline\n",
        "clf0 = LogisticRegression(max_iter=1000)\n",
        "\n",
        "clf0.fit(X_train, y_train)\n",
        "y_hat_test = clf0.predict(xx_test)\n",
        "\n",
        "acc_test = accuracy_score(yy_test, y_hat_test)\n",
        "f1_test = f1_score(yy_test, y_hat_test)\n",
        "PrecisionScore_test = precision_score(yy_test , y_hat_test)\n",
        "RecallScore_test = recall_score(yy_test , y_hat_test)\n",
        "g_mean_test = geometric_mean_score(yy_test, y_hat_test)\n",
        "\n",
        "#fpr, tpr, thresholds = roc_curve(yy_test, y_hat_test)\n",
        "auc = roc_auc_score(yy_test, y_hat_test)\n",
        "rounded_auc = round(auc,4)\n",
        "\n",
        "'''plt.figure(figsize=(12,6), dpi=600)\n",
        "\n",
        "plt.plot(fpr,tpr,linewidth=2, label=\"Baseline\" + \", auc=\"+str(rounded_auc))\n",
        "\n",
        "plt.legend(loc=4)\n",
        "plt.plot([0,1], [0,1], 'k--' )\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "#plt.savefig(str(name)+'.png', bbox_inches='tight')\n",
        "plt.show()  '''\n",
        "\n",
        "print(f\"Test Accuracy Score: {round(acc_test,4)}\")\n",
        "print(f\"Test f1 Score: {round(f1_test,4)}\")\n",
        "print(f\"Test Precision Score: {round(PrecisionScore_test,4)}\")\n",
        "print(f\"Test Recall Score: {round(RecallScore_test,4)}\")\n",
        "print(f\"Test GM Score: {round(g_mean_test,4)}\")\n",
        "print(f\"Test AUC Score: {rounded_auc}\")\n",
        "\n",
        "plot_confusion_matrix(clf0, xx_test, yy_test, cmap='Blues', normalize='true',\n",
        "                     display_labels=['No Tumor.', 'Tumor']);"
      ],
      "metadata": {
        "id": "SZpvLfPkCstn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Classifiers: 1. Logistic Regression 2. Naive Bayes 3. MLP 4. SVC 5. Ensemble of Classifiers"
      ],
      "metadata": {
        "id": "sdVT2dLdC01i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Logistic Regression"
      ],
      "metadata": {
        "id": "EuObMg40DHa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of grid searching key hyperparametres for logistic regression\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# define dataset\n",
        "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
        "# define models and parameters\n",
        "model = LogisticRegression()\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "penalty = ['l2']\n",
        "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
        "grid_result = grid_search.fit(X_train, y_train)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "bRtids7gCzw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------Top k-most confident predictions------------------------\n",
        "# Initiate iteration counter\n",
        "iterations = 0\n",
        "\n",
        "# Containers to hold f1_scores and # of pseudo-labels\n",
        "test_f1s = []\n",
        "test_accs = []\n",
        "test_precs = []\n",
        "test_recs = []\n",
        "#test_gmeans = []\n",
        "#test_aucs = []\n",
        "test_spss = []\n",
        "\n",
        "pseudo_labels = []\n",
        "\n",
        "# Assign value to initiate while loop\n",
        "high_prob = [1]\n",
        "\n",
        "# Loop will run until there are no more high-probability pseudo-labels\n",
        "while len(X_unlabeled) > 100:\n",
        "    # Fit classifier and make train/test predictions\n",
        "    clf1 = LogisticRegression(max_iter=1000)\n",
        "    clf1.fit(X_train, y_train)\n",
        "    y_hat_train = clf1.predict(X_train)\n",
        "    y_hat_test = clf1.predict(xx_test)\n",
        "    # Calculate and print iteration and scores\n",
        "\n",
        "    acc_test = accuracy_score(yy_test, y_hat_test)\n",
        "    f1_test = f1_score(yy_test, y_hat_test)\n",
        "    PrecisionScore_test = precision_score(yy_test , y_hat_test)\n",
        "    RecallScore_test = recall_score(yy_test , y_hat_test)\n",
        "    sps_test = specificity_score(yy_test, y_hat_test)\n",
        "\n",
        "    #fpr, tpr, thresholds = roc_curve(yy_test, y_hat_test)\n",
        "    #auc = roc_auc_score(yy_test, y_hat_test)\n",
        "    #rounded_auc = round(auc,4)\n",
        "\n",
        "    print(f\"Iteration {iterations}\")\n",
        "    print(f\"Test Accuracy Score: {round(acc_test,4)}\")\n",
        "    print(f\"Test f1 Score: {round(f1_test,4)}\")\n",
        "    print(f\"Test Precision Score: {round(PrecisionScore_test,4)}\")\n",
        "    print(f\"Test Recall Score: {round(RecallScore_test,4)}\")\n",
        "    print(f\"Test Specificity Score: {round(sps_test,4)}\")\n",
        "    #print(f\"Test AUC Score: {rounded_auc}\")\n",
        "\n",
        "    test_f1s.append(round(f1_test,4))\n",
        "    test_accs.append(round(acc_test,4))\n",
        "    test_precs.append(round(PrecisionScore_test,4))\n",
        "    test_recs.append(round(RecallScore_test,4))\n",
        "    test_spss.append(round(sps_test,4))\n",
        "\n",
        "    print(confusion_matrix(yy_test, y_hat_test))\n",
        "    print(classification_report(yy_test, y_hat_test))\n",
        "\n",
        "    # Generate predictions and probabilities for unlabeled data\n",
        "    print(f\"Now predicting labels for unlabeled data...\")\n",
        "\n",
        "    pred_probs = clf1.predict_proba(X_unlabeled)\n",
        "    preds = clf1.predict(X_unlabeled)\n",
        "    prob_0 = pred_probs[:,0]\n",
        "    prob_1 = pred_probs[:,1]\n",
        "    # Store predictions and probabilities in dataframe\n",
        "    df_pred_prob = pd.DataFrame([])\n",
        "    df_pred_prob['preds'] = preds\n",
        "    df_pred_prob['prob_0'] = prob_0\n",
        "    df_pred_prob['prob_1'] = prob_1\n",
        "    df_pred_prob.index = X_unlabeled.index\n",
        "\n",
        "    prob_mapped_df = pd.concat([X_unlabeled, df_pred_prob],axis=1)\n",
        "    c0_df = prob_mapped_df[(prob_mapped_df['preds'] == 0 )]\n",
        "    c1_df = prob_mapped_df[(prob_mapped_df['preds'] == 1 )]\n",
        "\n",
        "    c0_df = c0_df.sort_values(by ='prob_0' , ascending=False)\n",
        "    top_k_df0 = c0_df.iloc[:50]\n",
        "\n",
        "    c1_df = c1_df.sort_values(by ='prob_1' , ascending=False)\n",
        "    top_k_df1 = c1_df.iloc[:50]\n",
        "\n",
        "    high_prob = pd.concat([top_k_df0,top_k_df1], axis=0)\n",
        "    high_prob = high_prob.loc[:, high_prob.columns.intersection(['preds','prob_0','prob_1'])]\n",
        "    #print(high_prob.head())\n",
        "\n",
        "\n",
        "    print(f\"{len(high_prob)} high-probability predictions added to training data.\")\n",
        "\n",
        "    pseudo_labels.append(len(high_prob))\n",
        "\n",
        "    # Add pseudo-labeled data to training data\n",
        "    X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\n",
        "    y_train = pd.concat([y_train, high_prob.preds])\n",
        "    # Drop pseudo-labeled instances from unlabeled data\n",
        "    X_unlabeled = X_unlabeled.drop(index=high_prob.index)\n",
        "    print(f\"{len(X_unlabeled)} unlabeled instances remaining.\\n\")\n",
        "\n",
        "    # Update iteration counter\n",
        "    iterations += 1"
      ],
      "metadata": {
        "id": "6uhNsO7dDPEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
        "ax1.plot(range(iterations), test_f1s)\n",
        "ax1.set_ylabel('f1 Score')\n",
        "ax2.bar(x=range(iterations), height=pseudo_labels)\n",
        "ax2.set_ylabel('Pseudo-Labels Created')\n",
        "ax2.set_xlabel('# Iterations');\n",
        "\n",
        "plot_confusion_matrix(clf1, xx_test, yy_test, cmap='Blues', normalize='true',\n",
        "                     display_labels=['No Tumor.', 'Tumor']);"
      ],
      "metadata": {
        "id": "U-E4q4yASPMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Gaussian Naive Bayes"
      ],
      "metadata": {
        "id": "jed-0BagTgG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classifier = GaussianNB()\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
        "gs_NB = GridSearchCV(estimator=nb_classifier,\n",
        "                 param_grid=params_NB,\n",
        "                 cv=cv,   # use any cross validation technique\n",
        "                 verbose=1,\n",
        "                 scoring='accuracy')\n",
        "gs_NB.fit(X_train, y_train)\n",
        "\n",
        "gs_NB.best_params_"
      ],
      "metadata": {
        "id": "2XD4Ui9RSPIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------Top k-most confident predictions------------------------\n",
        "# Initiate iteration counter\n",
        "iterations = 0\n",
        "\n",
        "# Containers to hold f1_scores and # of pseudo-labels\n",
        "test_f1s = []\n",
        "test_accs = []\n",
        "test_precs = []\n",
        "test_recs = []\n",
        "test_spss = []\n",
        "\n",
        "pseudo_labels = []\n",
        "\n",
        "# Assign value to initiate while loop\n",
        "high_prob = [1]\n",
        "\n",
        "# Loop will run until there are no more high-probability pseudo-labels\n",
        "while len(X_unlabeled) > 100:\n",
        "    # Fit classifier and make train/test predictions\n",
        "    clf1 = GaussianNB(var_smoothing=0.8111308307896871)\n",
        "    clf1.fit(X_train, y_train)\n",
        "\n",
        "    y_hat_test = clf1.predict(xx_test)\n",
        "\n",
        "    # Calculate and print iteration # and f1 scores, and store f1 scores\n",
        "    acc_test = accuracy_score(yy_test, y_hat_test)\n",
        "    f1_test = f1_score(yy_test, y_hat_test)\n",
        "    PrecisionScore_test = precision_score(yy_test , y_hat_test)\n",
        "    RecallScore_test = recall_score(yy_test , y_hat_test)\n",
        "    sps_test = specificity_score(yy_test, y_hat_test)\n",
        "\n",
        "\n",
        "    print(f\"Iteration {iterations}\")\n",
        "    print(f\"Test Accuracy Score: {round(acc_test,4)}\")\n",
        "    print(f\"Test Precision Score: {round(PrecisionScore_test,4)}\")\n",
        "    print(f\"Test Recall Score: {round(RecallScore_test,4)}\")\n",
        "    print(f\"Test f1 Score: {round(f1_test,4)}\")\n",
        "    print(f\"Test Specificity Score: {round(sps_test,4)}\")\n",
        "\n",
        "    test_f1s.append(round(f1_test,4))\n",
        "    test_accs.append(round(acc_test,4))\n",
        "    test_precs.append(round(PrecisionScore_test,4))\n",
        "    test_recs.append(round(RecallScore_test,4))\n",
        "    test_spss.append(round(sps_test,4))\n",
        "\n",
        "    print(confusion_matrix(yy_test, y_hat_test))\n",
        "    print(classification_report(yy_test, y_hat_test))\n",
        "\n",
        "    # Generate predictions and probabilities for unlabeled data\n",
        "    print(f\"Now predicting labels for unlabeled data...\")\n",
        "\n",
        "    pred_probs = clf1.predict_proba(X_unlabeled)\n",
        "    preds = clf1.predict(X_unlabeled)\n",
        "    prob_0 = pred_probs[:,0]\n",
        "    prob_1 = pred_probs[:,1]\n",
        "    # Store predictions and probabilities in dataframe\n",
        "    df_pred_prob = pd.DataFrame([])\n",
        "    df_pred_prob['preds'] = preds\n",
        "    df_pred_prob['prob_0'] = prob_0\n",
        "    df_pred_prob['prob_1'] = prob_1\n",
        "    df_pred_prob.index = X_unlabeled.index\n",
        "\n",
        "    prob_mapped_df = pd.concat([X_unlabeled, df_pred_prob],axis=1)\n",
        "    c0_df = prob_mapped_df[(prob_mapped_df['preds'] == 0 )]\n",
        "    c1_df = prob_mapped_df[(prob_mapped_df['preds'] == 1 )]\n",
        "\n",
        "    c0_df = c0_df.sort_values(by ='prob_0' , ascending=False)\n",
        "    top_k_df0 = c0_df.iloc[:50]\n",
        "\n",
        "    c1_df = c1_df.sort_values(by ='prob_1' , ascending=False)\n",
        "    top_k_df1 = c1_df.iloc[:50]\n",
        "\n",
        "    high_prob = pd.concat([top_k_df0,top_k_df1], axis=0)\n",
        "    high_prob = high_prob.loc[:, high_prob.columns.intersection(['preds','prob_0','prob_1'])]\n",
        "    #print(high_prob.head())\n",
        "\n",
        "\n",
        "    print(f\"{len(high_prob)} high-probability predictions added to training data.\")\n",
        "\n",
        "    pseudo_labels.append(len(high_prob))\n",
        "\n",
        "    # Add pseudo-labeled data to training data\n",
        "    X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\n",
        "    y_train = pd.concat([y_train, high_prob.preds])\n",
        "    # Drop pseudo-labeled instances from unlabeled data\n",
        "    X_unlabeled = X_unlabeled.drop(index=high_prob.index)\n",
        "    print(f\"{len(X_unlabeled)} unlabeled instances remaining.\\n\")\n",
        "\n",
        "    # Update iteration counter\n",
        "    iterations += 1"
      ],
      "metadata": {
        "id": "1fFbH5a4SPFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
        "ax1.plot(range(iterations), test_f1s)\n",
        "ax1.set_ylabel('f1 Score')\n",
        "ax2.bar(x=range(iterations), height=pseudo_labels)\n",
        "ax2.set_ylabel('Pseudo-Labels Created')\n",
        "ax2.set_xlabel('# Iterations');\n",
        "\n",
        "plot_confusion_matrix(clf1, xx_test, yy_test, cmap='Blues', normalize='true',\n",
        "                     display_labels=['No Tumor.', 'Tumor']);"
      ],
      "metadata": {
        "id": "VYPn7iO5SPCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. MLP"
      ],
      "metadata": {
        "id": "qlyR5i28UH8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(max_iter=1000)\n",
        "\n",
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Best paramete set\n",
        "print('Best parameters found:\\n', clf.best_params_)\n",
        "\n",
        "# All results\n",
        "means = clf.cv_results_['mean_test_score']\n",
        "stds = clf.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
      ],
      "metadata": {
        "id": "d6X6yeIjSO_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------Top k-most confident predictions------------------------\n",
        "# Initiate iteration counter\n",
        "iterations = 0\n",
        "\n",
        "# Containers to hold f1_scores and # of pseudo-labels\n",
        "test_f1s = []\n",
        "test_accs = []\n",
        "test_precs = []\n",
        "test_recs = []\n",
        "test_spss = []\n",
        "\n",
        "pseudo_labels = []\n",
        "\n",
        "# Assign value to initiate while loop\n",
        "high_prob = [1]\n",
        "\n",
        "# Loop will run until there are no more high-probability pseudo-labels\n",
        "while len(X_unlabeled) > 100:\n",
        "    # Fit classifier and make train/test predictions\n",
        "    clf1 = MLPClassifier(activation= 'tanh', alpha= 0.05, hidden_layer_sizes= (100,), learning_rate= 'adaptive', solver= 'sgd', warm_start=True)\n",
        "    clf1.fit(X_train, y_train)\n",
        "\n",
        "    y_hat_test = clf1.predict(xx_test)\n",
        "\n",
        "    # Calculate and print iteration # and f1 scores, and store f1 scores\n",
        "    acc_test = accuracy_score(yy_test, y_hat_test)\n",
        "    f1_test = f1_score(yy_test, y_hat_test)\n",
        "    PrecisionScore_test = precision_score(yy_test , y_hat_test)\n",
        "    RecallScore_test = recall_score(yy_test , y_hat_test)\n",
        "    sps_test = specificity_score(yy_test, y_hat_test)\n",
        "\n",
        "\n",
        "    print(f\"Iteration {iterations}\")\n",
        "    print(f\"Test Accuracy Score: {round(acc_test,4)}\")\n",
        "    print(f\"Test Precision Score: {round(PrecisionScore_test,4)}\")\n",
        "    print(f\"Test Recall Score: {round(RecallScore_test,4)}\")\n",
        "    print(f\"Test f1 Score: {round(f1_test,4)}\")\n",
        "    print(f\"Test Specificity Score: {round(sps_test,4)}\")\n",
        "\n",
        "    test_f1s.append(round(f1_test,4))\n",
        "    test_accs.append(round(acc_test,4))\n",
        "    test_precs.append(round(PrecisionScore_test,4))\n",
        "    test_recs.append(round(RecallScore_test,4))\n",
        "    test_spss.append(round(sps_test,4))\n",
        "\n",
        "\n",
        "    print(confusion_matrix(yy_test, y_hat_test))\n",
        "    print(classification_report(yy_test, y_hat_test))\n",
        "\n",
        "    # Generate predictions and probabilities for unlabeled data\n",
        "    print(f\"Now predicting labels for unlabeled data...\")\n",
        "\n",
        "    pred_probs = clf1.predict_proba(X_unlabeled)\n",
        "    preds = clf1.predict(X_unlabeled)\n",
        "    prob_0 = pred_probs[:,0]\n",
        "    prob_1 = pred_probs[:,1]\n",
        "    # Store predictions and probabilities in dataframe\n",
        "    df_pred_prob = pd.DataFrame([])\n",
        "    df_pred_prob['preds'] = preds\n",
        "    df_pred_prob['prob_0'] = prob_0\n",
        "    df_pred_prob['prob_1'] = prob_1\n",
        "    df_pred_prob.index = X_unlabeled.index\n",
        "\n",
        "    prob_mapped_df = pd.concat([X_unlabeled, df_pred_prob],axis=1)\n",
        "    c0_df = prob_mapped_df[(prob_mapped_df['preds'] == 0 )]\n",
        "    c1_df = prob_mapped_df[(prob_mapped_df['preds'] == 1 )]\n",
        "\n",
        "    c0_df = c0_df.sort_values(by ='prob_0' , ascending=False)\n",
        "    top_k_df0 = c0_df.iloc[:50]\n",
        "\n",
        "    c1_df = c1_df.sort_values(by ='prob_1' , ascending=False)\n",
        "    top_k_df1 = c1_df.iloc[:50]\n",
        "\n",
        "    high_prob = pd.concat([top_k_df0,top_k_df1], axis=0)\n",
        "    high_prob = high_prob.loc[:, high_prob.columns.intersection(['preds','prob_0','prob_1'])]\n",
        "    #print(high_prob.head())\n",
        "\n",
        "\n",
        "    print(f\"{len(high_prob)} high-probability predictions added to training data.\")\n",
        "\n",
        "    pseudo_labels.append(len(high_prob))\n",
        "\n",
        "    # Add pseudo-labeled data to training data\n",
        "    X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\n",
        "    y_train = pd.concat([y_train, high_prob.preds])\n",
        "    # Drop pseudo-labeled instances from unlabeled data\n",
        "    X_unlabeled = X_unlabeled.drop(index=high_prob.index)\n",
        "    print(f\"{len(X_unlabeled)} unlabeled instances remaining.\\n\")\n",
        "\n",
        "    # Update iteration counter\n",
        "    iterations += 1"
      ],
      "metadata": {
        "id": "kMNCwjPvSO8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
        "ax1.plot(range(iterations), test_f1s)\n",
        "ax1.set_ylabel('f1 Score')\n",
        "ax2.bar(x=range(iterations), height=pseudo_labels)\n",
        "ax2.set_ylabel('Pseudo-Labels Created')\n",
        "ax2.set_xlabel('# Iterations');\n",
        "\n",
        "plot_confusion_matrix(clf1, xx_test, yy_test, cmap='Blues', normalize='true',\n",
        "                     display_labels=['No Tumor.', 'Tumor']);"
      ],
      "metadata": {
        "id": "P4-HhiKJSO5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['Accuracy','Precision','Recall', 'F1'])\n",
        "df['Accuracy'] = test_accs\n",
        "df['Precision'] = test_precs\n",
        "df['Recall'] = test_recs\n",
        "df['F1'] = test_f1s"
      ],
      "metadata": {
        "id": "GXywkr0QSO2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"20%performance.csv\")"
      ],
      "metadata": {
        "id": "fFXmYSE3SOyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. SVC"
      ],
      "metadata": {
        "id": "YqqnSosLUhtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# defining parameter range\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf']}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
        "\n",
        "# fitting the model for grid search\n",
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "W49eOUriSOv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------Top k-most confident predictions------------------------\n",
        "# Initiate iteration counter\n",
        "iterations = 0\n",
        "\n",
        "# Containers to hold f1_scores and # of pseudo-labels\n",
        "test_f1s = []\n",
        "test_accs = []\n",
        "test_precs = []\n",
        "test_recs = []\n",
        "test_spss = []\n",
        "\n",
        "pseudo_labels = []\n",
        "\n",
        "# Assign value to initiate while loop\n",
        "high_prob = [1]\n",
        "\n",
        "# Loop will run until there are no more high-probability pseudo-labels\n",
        "while len(X_unlabeled) > 100:\n",
        "    # Fit classifier and make train/test predictions\n",
        "    clf1 = SVC(probability=True)\n",
        "    clf1.fit(X_train, y_train)\n",
        "\n",
        "    y_hat_test = clf1.predict(xx_test)\n",
        "\n",
        "    # Calculate and print iteration # and f1 scores, and store f1 scores\n",
        "    acc_test = accuracy_score(yy_test, y_hat_test)\n",
        "    f1_test = f1_score(yy_test, y_hat_test)\n",
        "    PrecisionScore_test = precision_score(yy_test , y_hat_test)\n",
        "    RecallScore_test = recall_score(yy_test , y_hat_test)\n",
        "    sps_test = specificity_score(yy_test, y_hat_test)\n",
        "\n",
        "\n",
        "    print(f\"Iteration {iterations}\")\n",
        "    print(f\"Test Accuracy Score: {round(acc_test,4)}\")\n",
        "    print(f\"Test Precision Score: {round(PrecisionScore_test,4)}\")\n",
        "    print(f\"Test Recall Score: {round(RecallScore_test,4)}\")\n",
        "    print(f\"Test f1 Score: {round(f1_test,4)}\")\n",
        "    print(f\"Test Specificity Score: {round(sps_test,4)}\")\n",
        "\n",
        "    test_f1s.append(round(f1_test,4))\n",
        "    test_accs.append(round(acc_test,4))\n",
        "    test_precs.append(round(PrecisionScore_test,4))\n",
        "    test_recs.append(round(RecallScore_test,4))\n",
        "    test_spss.append(round(sps_test,4))\n",
        "\n",
        "\n",
        "    print(confusion_matrix(yy_test, y_hat_test))\n",
        "    print(classification_report(yy_test, y_hat_test))\n",
        "\n",
        "    # Generate predictions and probabilities for unlabeled data\n",
        "    print(f\"Now predicting labels for unlabeled data...\")\n",
        "\n",
        "    pred_probs = clf1.predict_proba(X_unlabeled)\n",
        "    preds = clf1.predict(X_unlabeled)\n",
        "    prob_0 = pred_probs[:,0]\n",
        "    prob_1 = pred_probs[:,1]\n",
        "    # Store predictions and probabilities in dataframe\n",
        "    df_pred_prob = pd.DataFrame([])\n",
        "    df_pred_prob['preds'] = preds\n",
        "    df_pred_prob['prob_0'] = prob_0\n",
        "    df_pred_prob['prob_1'] = prob_1\n",
        "    df_pred_prob.index = X_unlabeled.index\n",
        "\n",
        "    prob_mapped_df = pd.concat([X_unlabeled, df_pred_prob],axis=1)\n",
        "    c0_df = prob_mapped_df[(prob_mapped_df['preds'] == 0 )]\n",
        "    c1_df = prob_mapped_df[(prob_mapped_df['preds'] == 1 )]\n",
        "\n",
        "    c0_df = c0_df.sort_values(by ='prob_0' , ascending=False)\n",
        "    top_k_df0 = c0_df.iloc[:50]\n",
        "\n",
        "    c1_df = c1_df.sort_values(by ='prob_1' , ascending=False)\n",
        "    top_k_df1 = c1_df.iloc[:50]\n",
        "\n",
        "    high_prob = pd.concat([top_k_df0,top_k_df1], axis=0)\n",
        "    high_prob = high_prob.loc[:, high_prob.columns.intersection(['preds','prob_0','prob_1'])]\n",
        "    #print(high_prob.head())\n",
        "\n",
        "\n",
        "    print(f\"{len(high_prob)} high-probability predictions added to training data.\")\n",
        "\n",
        "    pseudo_labels.append(len(high_prob))\n",
        "\n",
        "    # Add pseudo-labeled data to training data\n",
        "    X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\n",
        "    y_train = pd.concat([y_train, high_prob.preds])\n",
        "    # Drop pseudo-labeled instances from unlabeled data\n",
        "    X_unlabeled = X_unlabeled.drop(index=high_prob.index)\n",
        "    print(f\"{len(X_unlabeled)} unlabeled instances remaining.\\n\")\n",
        "\n",
        "    # Update iteration counter\n",
        "    iterations += 1"
      ],
      "metadata": {
        "id": "VORClBRlSOse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
        "ax1.plot(range(iterations), test_f1s)\n",
        "ax1.set_ylabel('f1 Score')\n",
        "ax2.bar(x=range(iterations), height=pseudo_labels)\n",
        "ax2.set_ylabel('Pseudo-Labels Created')\n",
        "ax2.set_xlabel('# Iterations');\n",
        "\n",
        "plot_confusion_matrix(clf1, xx_test, yy_test, cmap='Blues', normalize='true',\n",
        "                     display_labels=['No Tumor.', 'Tumor']);"
      ],
      "metadata": {
        "id": "xheCLHdoSOot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Ensemble of LR, GNB, MLP"
      ],
      "metadata": {
        "id": "s4phjec2U-J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------Top k-most confident predictions------------------------\n",
        "# Initiate iteration counter\n",
        "iterations = 0\n",
        "\n",
        "# Containers to hold f1_scores and # of pseudo-labels\n",
        "train_f1s = []\n",
        "test_f1s = []\n",
        "pseudo_labels = []\n",
        "\n",
        "# Assign value to initiate while loop\n",
        "high_prob = [1]\n",
        "\n",
        "# Loop will run until there are no more high-probability pseudo-labels\n",
        "while (len(X_unlabeled) > 50):\n",
        "    # Fit classifier and make train/test predictions\n",
        "    clf1 = LogisticRegression(warm_start=True)\n",
        "    clf2 = GaussianNB()\n",
        "    clf3 = MLPClassifier(warm_start=True)\n",
        "    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('gnb', clf2), ('mlp', clf3)], voting='soft')\n",
        "    eclf1 = eclf1.fit(X_train, y_train)\n",
        "    #clf1 = LogisticRegression(warm_start=True)\n",
        "    #clf1 = SGDClassifier(loss='modified_huber')\n",
        "    #clf1.partial_fit(X_train, y_train, classes=np.unique(initial_y_train))\n",
        "    #clf1.fit(X_train, y_train)\n",
        "    y_hat_train_1 = eclf1.predict(X_train)\n",
        "    y_hat_test_1 = eclf1.predict(xx_test)\n",
        "    # Calculate and print iteration # and f1 scores, and store f1 scores\n",
        "    train_f1_1 = f1_score(y_train, y_hat_train_1)\n",
        "    test_f1_1 = f1_score(yy_test, y_hat_test_1)\n",
        "    #plot_confusion_matrix(clf, xx_test, yy_test, cmap='Blues', normalize='true',\n",
        "    #                 display_labels=['No Tumor.', 'Tumor']);\n",
        "    print(f\"Iteration {iterations}\")\n",
        "    #print(f\"Train f1: {train_f1}\")\n",
        "    print(f\"Test f1: {test_f1_1}\")\n",
        "    train_f1s.append(train_f1_1)\n",
        "    test_f1s.append(test_f1_1)\n",
        "    print(confusion_matrix(yy_test, y_hat_test_1))\n",
        "    print(classification_report(yy_test, y_hat_test_1))\n",
        "\n",
        "    # Generate predictions and probabilities for unlabeled data\n",
        "    print(f\"Now predicting labels for unlabeled data...\")\n",
        "\n",
        "    pred_probs = eclf1.predict_proba(X_unlabeled)\n",
        "    preds = eclf1.predict(X_unlabeled)\n",
        "    prob_0 = pred_probs[:,0]\n",
        "    prob_1 = pred_probs[:,1]\n",
        "\n",
        "    # Store predictions and probabilities in dataframe\n",
        "    df_pred_prob = pd.DataFrame([])\n",
        "    df_pred_prob['preds'] = preds\n",
        "    df_pred_prob['prob_0'] = prob_0\n",
        "    df_pred_prob['prob_1'] = prob_1\n",
        "    df_pred_prob.index = X_unlabeled.index\n",
        "\n",
        "    prob_mapped_df = pd.concat([X_unlabeled, df_pred_prob],axis=1)\n",
        "    c0_df = prob_mapped_df[(prob_mapped_df['preds'] == 0 )]\n",
        "    c1_df = prob_mapped_df[(prob_mapped_df['preds'] == 1 )]\n",
        "\n",
        "    c0_df = c0_df.sort_values(by ='prob_0' , ascending=False)\n",
        "    top_k_df0 = c0_df.iloc[:50]\n",
        "\n",
        "    c1_df = c1_df.sort_values(by ='prob_1' , ascending=False)\n",
        "    top_k_df1 = c1_df.iloc[:50]\n",
        "\n",
        "    high_prob = pd.concat([top_k_df0,top_k_df1], axis=0)\n",
        "    high_prob = high_prob.loc[:, high_prob.columns.intersection(['preds','prob_0','prob_1'])]\n",
        "    #print(high_prob.head())\n",
        "\n",
        "    # Separate predictions with > 99% probability\n",
        "    #high_prob = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.99],\n",
        "    #                       df_pred_prob.loc[df_pred_prob['prob_1'] > 0.99]],\n",
        "    #                      axis=0)\n",
        "\n",
        "    print(f\"{len(high_prob)} high-probability predictions added to training data.\")\n",
        "\n",
        "    pseudo_labels.append(len(high_prob))\n",
        "\n",
        "    # Add pseudo-labeled data to training data\n",
        "    X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\n",
        "    y_train = pd.concat([y_train, high_prob.preds])\n",
        "    # Drop pseudo-labeled instances from unlabeled data\n",
        "    X_unlabeled = X_unlabeled.drop(index=high_prob.index)\n",
        "    print(f\"{len(X_unlabeled)} unlabeled instances remaining.\\n\")\n",
        "\n",
        "    # Update iteration counter\n",
        "    iterations += 1"
      ],
      "metadata": {
        "id": "BD1Twk_MSOk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "id": "rqwmtSXlSOar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
        "ax1.plot(range(iterations), test_f1s)\n",
        "ax1.set_ylabel('f1 Score')\n",
        "ax2.bar(x=range(iterations), height=pseudo_labels)\n",
        "ax2.set_ylabel('Pseudo-Labels Created')\n",
        "ax2.set_xlabel('# Iterations');\n",
        "\n",
        "plot_confusion_matrix(eclf1, xx_test, yy_test, cmap='Blues', normalize='true',\n",
        "                     display_labels=['No Tumor.', 'Tumor']);"
      ],
      "metadata": {
        "id": "_azkqJsNSOON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}